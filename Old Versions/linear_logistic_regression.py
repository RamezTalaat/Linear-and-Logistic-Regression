# -*- coding: utf-8 -*-
"""Linear/Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AmrdjLaGTsvn8YoPu4kpMa3kU7yeb4Jd

This project's goal is to: build  a linear regression model and a logistic regression to predict loan decisions and amounts

**Solution Steps:**


1. Data Analysis on "loan_old.csv" dataset
2. Data Preprocessing
3. Linear Regression Model Fitting
4. Linear Regression Model Evalutaion
5. Logistic Regression Model Implementation and Fitting
6. Accuracy Evaluation Function
7. Preforming all previous steps on "loan_new.csv" dataset

**Step 1. Data Analysis on "loan_old.csv" dataset**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/loan_old.csv')
df.head()

# 1.i Checking for missing values
missing_count = df.isna().sum()
missing_count
# so we have some missing values in some columns ---->

#1.ii Checking feature types (categorical or numberical)
df.dtypes

df.nunique()
# this shows us the numerical vs categorical features , even if the categorical features are of type int64 or float64
# we will assume that numerical values are features which have high number of unique values
# So non categorical (numerical features) are ['Income','Coapplicant_Income','Max_Loan_Amount']

#1.iii check whether numerical features have the same scale
numerical_features = ['Income','Coapplicant_Income','Max_Loan_Amount']
numerical_columns_only = df[numerical_features]
numerical_columns_only.describe()
# so features do not have the same scale ---->

#1.iv visualize a pairplot between numercial columns
sns.pairplot(numerical_columns_only)
plt.show()

"""**Step 2. Data Preprocessing**"""

#2.i remove missing values records
df = df.dropna()
df.isna().sum()

#2.ii separate features and targets
features = df.drop(['Loan_ID','Max_Loan_Amount' ,'Loan_Status'] , axis =1 )    #takes all feature columns except the last 2 and the id column //as the id is not corelated with data
features.head()
targets = df[['Max_Loan_Amount','Loan_Status']] # takes the second last column as the target feature for linear regression model (continuous value)
targets.head()
#features.dtypes

#2.iv Categorical features encoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
encoded_features = features
#['Income','Coapplicant_Income','Max_Loan_Amount'] -> numerical , So we will encode all other columns
encoded_features['Gender'] = le.fit_transform(features['Gender'])
encoded_features['Married'] = le.fit_transform(features['Married'])
encoded_features['Dependents'] = le.fit_transform(features['Dependents'])
encoded_features['Education'] = le.fit_transform(features['Education'])
encoded_features['Loan_Tenor'] = le.fit_transform(features['Loan_Tenor'])
encoded_features['Credit_History'] = le.fit_transform(features['Credit_History'])
encoded_features['Property_Area'] = le.fit_transform(features['Property_Area'])
encoded_features.head()

encoded_features['Loan_Tenor'].unique()
encoded_features['Credit_History'].unique()

#2.v Categorical targets encoding
encoded_targets = targets
encoded_targets['Loan_Status'] = le.fit_transform(targets['Loan_Status'])
encoded_targets.head()

# #2.vi numerical features standerdization
from sklearn.preprocessing import StandardScaler # data is standerdized over 0 using mean and standard deviation
standard_scaler = StandardScaler()
encoded_features['Income'] = standard_scaler.fit_transform(encoded_features['Income'].values.reshape(-1, 1))
encoded_features['Coapplicant_Income'] = standard_scaler.fit_transform(encoded_features['Coapplicant_Income'].values.reshape(-1, 1))
encoded_features.head()

#2.iii Split into training and testing sets
from sklearn.model_selection import train_test_split
features_Train, features_Test, targets_Train, targets_Test = train_test_split(encoded_features, encoded_targets, test_size= 0.20, random_state=42) #20 of data set will be for testing
# random_state , to achieve that same data splited for later use in logistic regression model
features_Train.count() #410 rows
features_Test.count() #103 rows

"""**3. Linear Regression Model Fitting**"""

from sklearn import linear_model
linear_reg = linear_model.LinearRegression()
linear_reg.fit(features_Train, targets_Train['Max_Loan_Amount'])
linear_reg.score(features_Train, targets_Train['Max_Loan_Amount'])

"""**4. Linear Regression Model Evaluation**"""

linear_reg.score(features_Test, targets_Test['Max_Loan_Amount'])

Y_Pred = linear_reg.predict(features_Test)
Y_Test = np.ravel(targets_Test['Max_Loan_Amount'])
Y_Pred = np.ravel(Y_Pred)
Y_Test_Pred = pd.DataFrame({"Y_Test": Y_Test, "Y_Pred": Y_Pred})
Y_Test_Pred.head()

plt.figure(figsize=(10, 8))
Y_Test_Pred = Y_Test_Pred
plt.plot(Y_Test_Pred[:50])
plt.legend(["Actual", "Predicted"])

from LogisticRegression import LogisticRegression
theta_X = np.zeros(features_Train.shape[1])
itmes_number = features_Train.size/2.0
model = LogisticRegression(features_Train,targets_Train , 0 ,theta_X , 0.01,itmes_number)
model.predict()